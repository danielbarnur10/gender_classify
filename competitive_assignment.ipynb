{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Competitive Assignment\n",
    "An explanation this assignment could be found in the .pdf explanation document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preceding Step - import modules (packages)\n",
    "This step is necessary in order to use external modules (packages). <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"daniel barnur\"\n",
    "id= 313260770\n",
    "# imports for reading and writing (input & output) files:\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add whatever imports you need\n",
    "# YOUR CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "# function to split the data for cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# function for encoding categories\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "# the Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import hebrew_tokenizer as ht\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input files\n",
    "Reading input files for train annotated corpus (raw text data) corpus and for the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = '.' + os.sep + 'input' + os.sep + 'annotated_corpus_for_train_dup_female.xlsx'\n",
    "test_filename  = '.' + os.sep + 'input' + os.sep + 'corpus_for_test.xlsx'\n",
    "df_train = pd.read_excel(train_filename, 'corpus', index_col=None, na_values=['NA'])\n",
    "df_test  = pd.read_excel(test_filename,  'corpus', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>בשנה האחרונה חוויתי את מגפת הקורונה שהכריח את ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>בסמסטר קודם אני וכמה חברים ללימודים קבענו להיפ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>בשנה האחרונה למרות שלא היו יותר מידיי דברים לע...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...      m\n",
       "1  לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...      m\n",
       "2  השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...      m\n",
       "3  לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...      m\n",
       "4  יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...      m\n",
       "5  בשנה האחרונה חוויתי את מגפת הקורונה שהכריח את ...      m\n",
       "6  בסמסטר קודם אני וכמה חברים ללימודים קבענו להיפ...      m\n",
       "7  בשנה האחרונה למרות שלא היו יותר מידיי דברים לע...      f"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_example_id</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>כחלק ממסגרת ההתנדבות שלי במגלה אני הולך לפעמיי...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>לפני שנה החלטתי שאני רוצה להיות טייס, התחלתי ל...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>בתקופת הקורונה של תחילת החיסונים נגד קורונה, א...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_example_id                                              story\n",
       "0                0  כחלק ממסגרת ההתנדבות שלי במגלה אני הולך לפעמיי...\n",
       "1                1  לפני שנה החלטתי שאני רוצה להיות טייס, התחלתי ל...\n",
       "2                2  בתקופת הקורונה של תחילת החיסונים נגד קורונה, א..."
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your implementation:\n",
    "Write your code solution in the following code-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#function to countVectorizer to \"tokenizer\"\n",
    "def my_tokenizer(hebrew_text):\n",
    "    # tokenize returns a generator! \n",
    "    \n",
    "    hebrew_text2= hebrew_text. \n",
    "    return ht.tokenize(hebrew_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# creates a list of stop words from stop_word_list_heb_example.txt\n",
    "my_file = open(\"./stop_word_list_heb_example.txt\", encoding=\"utf8\")\n",
    "content = my_file.read()\n",
    "content_stop= content. split(\"\\n\");\n",
    "my_file. close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting our TRAIN set to two groups\n",
    "# train_set=df_train.tail(407-72)\n",
    "# validation_set=df_train.head(72)\n",
    "train_set ,validation_set = train_test_split(df_train,test_size=0.2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pipline with countVector includes stop words max features and tokens on our train set\n",
    "# we normalize (l1) it and then use naive bayes\n",
    "# stop_words= content_stop,\n",
    "text_clf_nb = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=content_stop,max_features=50000, tokenizer=my_tokenizer)),\n",
    "    ('norm', preprocessing.Normalizer(norm='l1')),\n",
    "    ('clf', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(max_features=50000,\n",
       "                                 stop_words=['לא', 'את', 'זה', 'של', 'על',\n",
       "                                             'אני', 'מה', 'עם', 'הוא', 'כל',\n",
       "                                             'אבל', 'גם', 'לי', 'יש', 'אם',\n",
       "                                             'אתה', 'רק', 'או', 'היה', 'אין',\n",
       "                                             'יותר', 'אז', 'היא', 'כי', 'לך',\n",
       "                                             'טוב', 'הם', 'מי', 'כמו', 'שלא', ...],\n",
       "                                 tokenizer=<function my_tokenizer at 0x0000028BD1F1F4C0>)),\n",
       "                ('norm', Normalizer(norm='l1')), ('clf', SVC())])"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_nb.fit(train_set.story,train_set.gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using on validation set to test our model\n",
    "predict = text_clf_nb.predict(validation_set.story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8260869565217391"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predict == validation_set.gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.89      0.53      0.67        30\n",
      "           m       0.81      0.97      0.88        62\n",
      "\n",
      "    accuracy                           0.83        92\n",
      "   macro avg       0.85      0.75      0.77        92\n",
      "weighted avg       0.84      0.83      0.81        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# built in classification report with f1_score for f and m and also macro avg for our Average_F1 of both\n",
    "print(metrics.classification_report(validation_set.gender, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another evaluation of the f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the Confusion Matrix \n",
    "# the function is fitted to the gender we need to check f | m\n",
    "def perf_measure(y_actual, y_hat, g1, g2):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==g1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==g1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==g2:\n",
    "            TN += 1\n",
    "        if y_hat[i]==g2 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    " \n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall tp / (tp + fn) \n",
    "# precision tp / (tp + fp)\n",
    "# Evaluation of F1_Score by gender\n",
    "def evaluateF1(y_test, y_predict, gender):\n",
    "    if gender=='m':\n",
    "            tp, fp, TN, fn  = perf_measure(y_test, y_predict,'m','f')\n",
    "    else:\n",
    "        tp, fp, TN, fn  = perf_measure(y_test, y_predict,'f','m')\n",
    "    precision = tp / (tp + fp)\n",
    "    recall= tp / (tp + fn) \n",
    "    f1=2*(precision * recall)/(precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male 0.8823529411764706\n",
      "female 0.6666666666666667\n",
      "AVG F1 0.7745098039215687\n"
     ]
    }
   ],
   "source": [
    "# evaluation f1 score\n",
    "# criteria of the function @function perf_measure is to be a list\n",
    "toList=list(validation_set.gender)\n",
    "f1_male = evaluateF1(toList,predict, 'm')\n",
    "f1_female = evaluateF1(toList,predict, 'f')\n",
    "# metrics.f1_score(df_train.gender.tail(182), predicted,average='weighted')\n",
    "print(\"male\",f1_male)\n",
    "print(\"female\",f1_female)\n",
    "Average_f1=(f1_male + f1_female)/2\n",
    "print(\"AVG F1\",Average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally using our pipline on the test set and hoping for good..\n",
    "docs_test = df_test.story\n",
    "predicted = text_clf_nb.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output to csv\n",
    "After you're done save your output to the 'classification_results.csv' csv file.<br/>\n",
    "We assume that the dataframe with your results contain the following columns:\n",
    "* column 1 (left column): 'test_example_id'  - the same id associated to each of the test stories to be predicted.\n",
    "* column 2 (right column): 'predicted_category' - the predicted gender value for each of the associated story. \n",
    "\n",
    "Assuming your predicted values are in the `df_predicted` dataframe, you should save you're results as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the results into a dataframe\n",
    "df_predicted = pd.DataFrame ({'test_example_id': df_test.index,'predicted_category': predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted.to_csv('classification_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender=df_train.gender\n",
    "story = df_train.story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 50)\n",
      "(291,)\n",
      "(73, 50)\n",
      "(73,)\n"
     ]
    }
   ],
   "source": [
    "# creating countVector with stop words max features and tokens on our train set\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=content_stop,max_features=50,min_df=2,tokenizer=my_tokenizer)\n",
    "\n",
    "x=count_vect.fit_transform(story)\n",
    "X_train_normalized = preprocessing.normalize(x, norm='l1')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(gender)\n",
    "\n",
    "\n",
    "# split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_train_normalized, y, test_size=0.2)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7808219178082192\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "print(nb.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_counts = count_vect.fit_transform(train_set.story)\n",
    "X_train_counts.shape\n",
    "my_matrix = X_train_counts.toarray()\n",
    "# X_train_counts.vocabulary_.get(\"בן זוגי\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pipline\n",
    "X_train_normalized = preprocessing.normalize(X_train_counts, norm='l1')\n",
    "#X_train_normalized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  naive bayes classifier\n",
    "clf = MultinomialNB().fit(X_train_normalized, train_set.story)\n",
    "# count_vect.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
